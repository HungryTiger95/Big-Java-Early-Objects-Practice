Logarithmic time estimates arise from algorithms that cut work in half in each step.

In particular, when you use sorting or binary search in a phase of an algorithm, you
will encounter logarithmic time in the big-Oh estimates.

Consider this idea for improving our algorithm for finding the most frequent element.
Suppose we first sort the array:

		8 7 5 5 7 7 5 4 ---> 4 5 5 7 7 7 8
		
That costs O(n log(n)) time. If we can complete the algorithm in O(n) time, we will
have found a better algorithm than the O(n^2) algorithm of the preceding sections.
To see why this is possible, imagine traversing the sorted array. As long as you find a value that was equal to its predecessor, you increment a counter. When you find a different value, save the counter and start counting anew:

		a: 4 5 5 7 7 7 8
   counts: 1 1 2 1 2 3 1
   
Or in code,

		int count = 0
		for(int i = 0; i < a.length; i++)
		{
			count++
			if(i == a.length - 1 || a[i] != a[i -1])
			{
				counts[i] = count;
				count = 0;
			}
		}
		
That's a constant amount of work per iteration, even though it visits two elements. 2n
is sitll O(n). Thus, we can compute the counts in O(n) time from a sorted array.
The entire algorithm is now O(n log(n)).